{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_Fr_jmSoc52"
      },
      "source": [
        "### NNDL baseline trainings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pXRiu5Uons_"
      },
      "source": [
        "#### Download the kaggle datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUgTm-UbocWs",
        "outputId": "e8fc09de-68fa-4966-e062-f5a47bc86ae9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-14 01:21:01--  https://www.dropbox.com/s/c2dvapqb613ukhw/Released_Data-20221201T215316Z-001.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.6.18, 2620:100:6019:18::a27d:412\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.6.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/c2dvapqb613ukhw/Released_Data-20221201T215316Z-001.zip [following]\n",
            "--2022-12-14 01:21:02--  https://www.dropbox.com/s/raw/c2dvapqb613ukhw/Released_Data-20221201T215316Z-001.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucd8cafadcaf6d02a8e3fe652c90.dl.dropboxusercontent.com/cd/0/inline/BymsVuZSeWHgTBDtdqrS5V6oY1kc2xjBR2tlXBTMm0gbGzVq-DxBdXH-L7R0gbQCdLLwutFYm3BE1OOsKJSZodpjAWlnF7KiEsL15IRIehXBU8EYuWRhygRKpIQPwBSGKQX5nBqbn_myFr8MWJMiRSpqwX0InX0mX5ovdGeMPDOwew/file# [following]\n",
            "--2022-12-14 01:21:02--  https://ucd8cafadcaf6d02a8e3fe652c90.dl.dropboxusercontent.com/cd/0/inline/BymsVuZSeWHgTBDtdqrS5V6oY1kc2xjBR2tlXBTMm0gbGzVq-DxBdXH-L7R0gbQCdLLwutFYm3BE1OOsKJSZodpjAWlnF7KiEsL15IRIehXBU8EYuWRhygRKpIQPwBSGKQX5nBqbn_myFr8MWJMiRSpqwX0InX0mX5ovdGeMPDOwew/file\n",
            "Resolving ucd8cafadcaf6d02a8e3fe652c90.dl.dropboxusercontent.com (ucd8cafadcaf6d02a8e3fe652c90.dl.dropboxusercontent.com)... 162.125.6.15, 2620:100:6019:15::a27d:40f\n",
            "Connecting to ucd8cafadcaf6d02a8e3fe652c90.dl.dropboxusercontent.com (ucd8cafadcaf6d02a8e3fe652c90.dl.dropboxusercontent.com)|162.125.6.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/Bynsmtnt8KjjXqlgM0597NEyMb-pxzhAb_EZZqB2OnXtVIUyRIfwYaYqOQga-v8LdUQzJ1MBbqIr9P9gEEjPd40K0cV7Bz-XcEBJq1tcnl7Byk43djt-szVOScJyZGHPQkDIOGPAY3n7p_FPkp6bccjGdG0Npx1HZDHQMTJeAnghUrJYlHthIFMbeyfuMKoWcUN_b7OacuO5s65icfCmBlNG1nfuT-QtnYsIG3XLQ4EDla3SaCq97ONh7EIYuuHs087-_ly3dwWqcZ4cnJAvECfr6J0DICiWZ76U2MPlcb1ssxb7J_dgXb7BNgAR5M3QEJ3CSxeTkDztDmfaMuyS1nIVgrLAZA6GK7Oi-PJWEOJL_YxBJu4HhDyaXzEp8Ka2UTINKTDQZaN_p0GTUYckh245IRTF3er6mv0xwnKKcXAIig/file [following]\n",
            "--2022-12-14 01:21:02--  https://ucd8cafadcaf6d02a8e3fe652c90.dl.dropboxusercontent.com/cd/0/inline2/Bynsmtnt8KjjXqlgM0597NEyMb-pxzhAb_EZZqB2OnXtVIUyRIfwYaYqOQga-v8LdUQzJ1MBbqIr9P9gEEjPd40K0cV7Bz-XcEBJq1tcnl7Byk43djt-szVOScJyZGHPQkDIOGPAY3n7p_FPkp6bccjGdG0Npx1HZDHQMTJeAnghUrJYlHthIFMbeyfuMKoWcUN_b7OacuO5s65icfCmBlNG1nfuT-QtnYsIG3XLQ4EDla3SaCq97ONh7EIYuuHs087-_ly3dwWqcZ4cnJAvECfr6J0DICiWZ76U2MPlcb1ssxb7J_dgXb7BNgAR5M3QEJ3CSxeTkDztDmfaMuyS1nIVgrLAZA6GK7Oi-PJWEOJL_YxBJu4HhDyaXzEp8Ka2UTINKTDQZaN_p0GTUYckh245IRTF3er6mv0xwnKKcXAIig/file\n",
            "Reusing existing connection to ucd8cafadcaf6d02a8e3fe652c90.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21340222 (20M) [application/zip]\n",
            "Saving to: ‘released_data.zip’\n",
            "\n",
            "released_data.zip   100%[===================>]  20.35M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-12-14 01:21:03 (198 MB/s) - ‘released_data.zip’ saved [21340222/21340222]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Download the dataset from Dropbox\n",
        "!wget -O released_data.zip \"https://www.dropbox.com/s/c2dvapqb613ukhw/Released_Data-20221201T215316Z-001.zip?dl=0\"\n",
        "\n",
        "#Unzip the train, test and other datasets\n",
        "!unzip -q released_data.zip\n",
        "!unzip -q ./Released_Data/test_shuffle.zip\n",
        "!unzip -q ./Released_Data/train_shuffle.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmvpcuP6s0oF"
      },
      "source": [
        "#### Dataset and Dataloader preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbihYhyYvSxc"
      },
      "source": [
        "##### Train and Validation data splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahJj7UBNvQ0C",
        "outputId": "fd5fe41b-65d6-493f-be69-fdb3baf1c0f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common names:  []\n",
            "Error names:  []\n",
            "Total number of images:  6472\n",
            "Number of training images:  (5738, 3)\n",
            "Number of validation images:  (734, 3)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "full_imgs = np.array(os.listdir('./train_shuffle'))\n",
        "full_labels = pd.read_csv('./Released_Data/train_data.csv').to_numpy()\n",
        "\n",
        "labels_list = [set(),set(),set()]\n",
        "\n",
        "#prepare lists based on the super classes\n",
        "labels_list = [{}, {}, {}]\n",
        "val_labels_list = [{}, {}, {}]\n",
        "\n",
        "for i in range(len(full_labels)):\n",
        "    if full_labels[i][2] in labels_list[full_labels[i][1]]:\n",
        "        labels_list[full_labels[i][1]][full_labels[i][2]] += 1\n",
        "    else:\n",
        "        labels_list[full_labels[i][1]][full_labels[i][2]] = 1\n",
        "        val_labels_list[full_labels[i][1]][full_labels[i][2]] = 0\n",
        "\n",
        "# Check the number of times each sub class appears\n",
        "# print(dict(sorted(labels_list[0].items(), key=lambda item: item[1])))\n",
        "# print(dict(sorted(labels_list[1].items(), key=lambda item: item[1])))\n",
        "# print(dict(sorted(labels_list[2].items(), key=lambda item: item[1])))\n",
        "\n",
        "#Splitting the train and validation datasets (90 - 10 split)\n",
        "train_imgs = []\n",
        "val_imgs = []\n",
        "\n",
        "for i in range(len(full_labels)):\n",
        "    if val_labels_list[full_labels[i][1]][full_labels[i][2]] <= (0.1)*(labels_list[full_labels[i][1]][full_labels[i][2]]):\n",
        "        val_labels_list[full_labels[i][1]][full_labels[i][2]] += 1\n",
        "        val_imgs.append(full_labels[i][0])\n",
        "    else:\n",
        "        train_imgs.append(full_labels[i][0])\n",
        "\n",
        "#Check if there is any common image between training and validation datasets\n",
        "common_names = [name for name in train_imgs if name in val_imgs]\n",
        "print(\"Common names: \", common_names)\n",
        "\n",
        "train_data = full_labels[np.isin(full_labels[:, 0], train_imgs)]\n",
        "val_data = full_labels[np.isin(full_labels[:, 0], val_imgs)]\n",
        "\n",
        "#Check if there is any name missing from the training dataset\n",
        "ver_train_names = train_data[:, 0]\n",
        "error_names = [name for name in ver_train_names if name not in train_imgs]\n",
        "print(\"Error names: \", error_names)\n",
        "\n",
        "print(\"Total number of images: \", len(full_imgs))\n",
        "print(\"Number of training images: \", train_data.shape)\n",
        "print(\"Number of validation images: \", val_data.shape)\n",
        "# print(type(train_data[0][0]))\n",
        "# print(type(train_data[0][1]))\n",
        "# print(type(val_data[0][0]))\n",
        "# print(type(val_data[0][1]))\n",
        "\n",
        "\n",
        "# --------Code to check the distribution of super and sub classes in the training and validation datasets---------\n",
        "# train_sub_dict = {}\n",
        "# train_super_dict = {}\n",
        "# val_sub_dict = {}\n",
        "# val_super_dict = {}\n",
        "\n",
        "# for i in range(full_labels.shape[0]):\n",
        "#     name = full_labels[i][0]\n",
        "#     super_class = full_labels[i][1]\n",
        "#     sub_class = full_labels[i][2]\n",
        "\n",
        "#     if name in val_data[:, 0]:\n",
        "#         if super_class in val_super_dict:\n",
        "#             val_super_dict[super_class] += 1\n",
        "#         else:\n",
        "#             val_super_dict[super_class] = 1\n",
        "        \n",
        "#         if sub_class in val_sub_dict:\n",
        "#             val_sub_dict[sub_class] += 1\n",
        "#         else:\n",
        "#             val_sub_dict[sub_class] = 1\n",
        "#     else:\n",
        "#         if super_class in train_super_dict:\n",
        "#             train_super_dict[super_class] += 1\n",
        "#         else:\n",
        "#             train_super_dict[super_class] = 1\n",
        "        \n",
        "#         if sub_class in train_sub_dict:\n",
        "#             train_sub_dict[sub_class] += 1\n",
        "#         else:\n",
        "#             train_sub_dict[sub_class] = 1\n",
        "\n",
        "# print(\"Super Class distribution:\")\n",
        "# for k in val_super_dict:\n",
        "#     print(\"Class: \", k, \" Validation: \", val_super_dict[k], \" Train: \", train_super_dict[k], \" Ratio: \", train_super_dict[k]/val_super_dict[k])\n",
        "\n",
        "# print(\"------------------------------\")\n",
        "# print(\"Sub Class distribution:\")\n",
        "# min_val = 10000\n",
        "# max_val = -1\n",
        "# for k in val_sub_dict:\n",
        "#     min_val = min(min_val, train_sub_dict[k]/val_sub_dict[k])\n",
        "#     max_val = max(max_val, train_sub_dict[k]/val_sub_dict[k])\n",
        "#     print(\"Class: \", k, \" Validation: \", val_sub_dict[k], \" Train: \", train_sub_dict[k], \" Ratio: \", train_sub_dict[k]/val_sub_dict[k])\n",
        "\n",
        "# print(\"Minimum Sub Class ratio: \", min_val)\n",
        "# print(\"Maximum Sub Class ratio: \", max_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_1w3N4F7qbb"
      },
      "source": [
        "##### Information about the images in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Zx9-8L0G7qw_",
        "outputId": "ac5b5baa-2bf7-4b8f-8b17-fb25d8fbe1f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "328.jpg (8, 8, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=8x8 at 0x7FAB637FE7F0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAA00lEQVR4nAHIADf/AaORfQwND9re4BUYHA0PEM7Qzzg0NOnq5wIcHBzg4ODU1NT7+/v7+vzb2tzd394NDQ0E0tPVHB0fFxwg9fn62dvaDw8PDQ0P+vv9AgwMDvj7+g4OEOTk5ujr7CotLv3+/+vt7gIHCAnp6uv9/wHq6+zn6OoODxHq7e7z9fYE7O7w+/7/JCUmSwkKz9LWDQ8R2tra///9BBQWF9XZ2NHT1DY6O/8ECOHh4e7x8Pv7+wF6cGQYGhv2+vz6/QG9v78KCwskISHg4d7utXBL0z4qVgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.jpg (8, 8, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=8x8 at 0x7FAB4E78C400>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAA00lEQVR4nAHIADf/AaScp9rb18bJwR4iGywsKszP0Dg3PxUXGwIDAwPw8PA0NDTg3d7u7uzm5eMLCwn9+vkE/gD/GhwbGhsU7u/p2tjYIyEg5+TiISEiAvT09BgYGPv7+xISEuHf3yonJNHLxgkD/wIHCgkICAj////e3dsfHRsGAP7t6OIMB/8CAwMDCAsK8vLy+fb3GBUU+PT05N/b5+PcBPL08QkLB9XV1fv7++Dd3g8HAvPw/+/q5gQRExIDBAH09PTs7OwHAfn7/B05NSAxLSoIA2XCXw3xngAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4713.jpg (8, 8, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=8x8 at 0x7FAB4E7952B0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAAgAAAAICAIAAABLbSncAAAAyUlEQVR4nAXBW0rDQBQA0PROZjrNk6bFCIIkC6gb0G7OLdkfKVR3ICIo+CP5ERFSm8yjt8ncjudM7jcPzrlh9EmeGXvSR8tCiYggQ74sFheLZfvze3ZU5MWx12mcAPb64+UVOyUCkB4i4GEwkQEHNtB28/j59n59eZXJCDs1j9P+7wDzKE+Y+P5qbHuI+RQ7PQM+agt0Gta3d0D+efeE2nAmaDznaQZKqaqq6rpumqbd76MkHslN5QyEEN771eqmLEtrkTFGRMaYf9zaYuvZih1sAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "full_test_imgs = os.listdir(\"./test_shuffle\")\n",
        "\n",
        "train_img = cv2.imread(os.path.join('train_shuffle', train_data[0][0]), cv2.IMREAD_UNCHANGED)\n",
        "print(train_data[0][0], train_img.shape)\n",
        "cv2_imshow(train_img)\n",
        "\n",
        "val_img = cv2.imread(os.path.join('train_shuffle', val_data[0][0]), cv2.IMREAD_UNCHANGED)\n",
        "print(val_data[0][0], val_img.shape)\n",
        "cv2_imshow(val_img)\n",
        "\n",
        "test_img = cv2.imread(os.path.join('train_shuffle', full_test_imgs[0]), cv2.IMREAD_UNCHANGED)\n",
        "print(full_test_imgs[0], test_img.shape)\n",
        "cv2_imshow(test_img)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrEdW8Li5-DY"
      },
      "source": [
        "##### KaggleDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "P9rfTIHYs0-B"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "class KaggleDataset(nn.Module):\n",
        "    def __init__(self, mode = 'train'):\n",
        "        super(KaggleDataset, self).__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.dataset = train_data\n",
        "        else:\n",
        "            self.dataset = val_data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataset[idx][0]\n",
        "        img = cv2.imread(os.path.join('train_shuffle', img_name))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    #convert image from BGR to RGB format\n",
        "\n",
        "        super_class = torch.tensor(self.dataset[idx][1], dtype = torch.float32)\n",
        "        sub_class = torch.tensor(self.dataset[idx][2], dtype = torch.float32)\n",
        "\n",
        "        apply_transform = self.transform_data()\n",
        "        image = apply_transform(image = img)['image']\n",
        "\n",
        "        return image, super_class, sub_class\n",
        "\n",
        "    def transform_data(self):\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            transform_data = A.Compose(\n",
        "              [\n",
        "                  #always resize the image to 329x224\n",
        "                  #A.Resize(height = 329, width= 224, interpolation = cv2.INTER_AREA, p=1),\n",
        "                  A.HorizontalFlip(p=0.4),  \n",
        "                  A.ShiftScaleRotate(shift_limit=0.025, scale_limit=0, rotate_limit=15, p=0.5),\n",
        "                  #A.RandomCrop(height = 224, width = 224, p=1),\n",
        "                  #randomly change brightness, contrast, and saturation of the image 50% of the time\n",
        "                  A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue = 0, p=0.5), \n",
        "                  A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1), \n",
        "                  ToTensorV2(p=1),\n",
        "              ])\n",
        "        else:     #augmentations during validation and testing\n",
        "          transform_data = A.Compose(\n",
        "          [\n",
        "              #always resize the image to 329x224\n",
        "              #A.Resize(height = 329, width = 224, p=1),   \n",
        "              A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1),\n",
        "              ToTensorV2(p=1),\n",
        "          ])\n",
        "    \n",
        "        return transform_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CI1Srqj-GkHs"
      },
      "source": [
        "##### KaggleDataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekKlmYMuGmnu",
        "outputId": "dc92059f-4a2a-42d6-dd33-74f69c3a1c5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = KaggleDataset(mode='train')\n",
        "val_dataset = KaggleDataset(mode='val')\n",
        "    \n",
        "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True, num_workers = 8, pin_memory = True)\n",
        "val_loader = DataLoader(dataset = val_dataset, batch_size = 64, shuffle = False)\n",
        "\n",
        "# print(len(train_loader))\n",
        "# print(len(val_loader))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4R13Pg7NRAW"
      },
      "source": [
        "# Pre-train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgdGTIfUNRAW",
        "outputId": "ac885a0a-84fc-4ae5-cb96-c014047b578f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: efficientnet in /usr/local/lib/python3.8/dist-packages (1.1.1)\n",
            "Requirement already satisfied: keras-applications<=1.0.8,>=1.0.7 in /usr/local/lib/python3.8/dist-packages (from efficientnet) (1.0.8)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (from efficientnet) (0.18.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.8/dist-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.21.6)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet) (2022.10.10)\n",
            "Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet) (1.7.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet) (2.9.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet) (2.8.8)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet) (1.4.1)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image->efficientnet) (7.1.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image->efficientnet) (1.15.0)\n",
            "Downloading data from https://github.com/Callidior/keras-applications/releases/download/efficientnet/efficientnet-b6_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5\n",
            "165527152/165527152 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "\n",
        "width = 32\n",
        "height = 32\n",
        "input_shape = (height, width, 3)\n",
        "\n",
        "# uncomment to install the package\n",
        "!pip install -U efficientnet\n",
        "import efficientnet.keras as efn\n",
        "conv_base = efn.EfficientNetB6(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "batch_size = 4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsbQxK0_NRAX"
      },
      "source": [
        "Set up EfficientNet Training Job"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RRNw5ceNRAY",
        "outputId": "f95b7390-68ac-4fc2-d7f1-982d1b16d109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "building netowrk for 3 classes\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " efficientnet-b6 (Functional  (None, 1, 1, 2304)       40960136  \n",
            " )                                                               \n",
            "                                                                 \n",
            " gap (GlobalMaxPooling2D)    (None, 2304)              0         \n",
            "                                                                 \n",
            " dropout_out (Dropout)       (None, 2304)              0         \n",
            "                                                                 \n",
            " fc_out (Dense)              (None, 3)                 6915      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 40,967,051\n",
            "Trainable params: 40,742,619\n",
            "Non-trainable params: 224,432\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import os, os.path\n",
        "import keras\n",
        "keras.__version__\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import optimizers\n",
        "epochs = 10\n",
        "NUM_TRAIN = len(train_loader)\n",
        "NUM_TEST = len(val_loader)\n",
        "dropout_rate = 0.2\n",
        "num_classes = 3\n",
        "print('building netowrk for ' + str(num_classes) + ' classes')\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(conv_base)\n",
        "model.add(layers.GlobalMaxPooling2D(name=\"gap\"))\n",
        "# model.add(layers.Flatten(name=\"flatten\"))\n",
        "if dropout_rate > 0:\n",
        "    model.add(layers.Dropout(dropout_rate, name=\"dropout_out\"))\n",
        "# model.add(layers.Dense(256, activation='relu', name=\"fc1\"))\n",
        "model.add(layers.Dense(3, activation='softmax', name=\"fc_out\"))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oAYN91PPNRAZ"
      },
      "source": [
        "Run EfficientNet Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "3u4Kkm7iNRAa",
        "outputId": "1d4533e9-e97e-4513-ab29-dfc3d1986c23"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-834ec08786b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m      7\u001b[0m       \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m       \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mNUM_TRAIN\u001b[0m \u001b[0;34m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    983\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \"input: {}, {}\".format(\n",
            "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'torch.utils.data.dataloader.DataLoader'>, <class 'NoneType'>"
          ]
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['acc'])\n",
        "\n",
        "\n",
        "history = model.fit(\n",
        "      train_loader,\n",
        "      steps_per_epoch= NUM_TRAIN //batch_size,\n",
        "      epochs=epochs,\n",
        "      validation_data=val_loader,\n",
        "      validation_steps= NUM_TEST //batch_size,\n",
        "      verbose=1,\n",
        "      use_multiprocessing=True,\n",
        "      workers=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qem0mto9NRAa"
      },
      "source": [
        "Examine EfficientNet Training Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcFCStBZNRAb"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_x = range(len(acc))\n",
        "\n",
        "plt.plot(epochs_x, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs_x, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs_x, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs_x, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.3 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}