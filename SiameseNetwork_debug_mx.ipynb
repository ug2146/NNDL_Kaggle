{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download kaggle dataset"
      ],
      "metadata": {
        "id": "L6bkpa-el91Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import shutil\n",
        "shutil.rmtree('./content/Released_Data', ignore_errors=True)\n",
        "shutil.rmtree('./content/sample_data', ignore_errors=True)\n",
        "shutil.rmtree('./content/test_shuffle', ignore_errors=True)\n",
        "shutil.rmtree('./content/train_shuffle', ignore_errors=True)\n",
        "'''\n",
        "#Download the dataset from Dropbox\n",
        "!wget -O released_data.zip \"https://www.dropbox.com/s/c2dvapqb613ukhw/Released_Data-20221201T215316Z-001.zip?dl=0\"\n",
        "\n",
        "#Unzip the train, test and other datasets\n",
        "!unzip -q released_data.zip\n",
        "!unzip -q ./Released_Data/test_shuffle.zip\n",
        "!unzip -q ./Released_Data/train_shuffle.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3Ucr3bkmEhL",
        "outputId": "9ffe6953-433b-4011-b283-be1a76a73e68"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-16 17:10:24--  https://www.dropbox.com/s/c2dvapqb613ukhw/Released_Data-20221201T215316Z-001.zip?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.81.18, 2620:100:6031:18::a27d:5112\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.81.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /s/raw/c2dvapqb613ukhw/Released_Data-20221201T215316Z-001.zip [following]\n",
            "--2022-12-16 17:10:24--  https://www.dropbox.com/s/raw/c2dvapqb613ukhw/Released_Data-20221201T215316Z-001.zip\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc1fe3baf1750fc0b91651b53016.dl.dropboxusercontent.com/cd/0/inline/ByvyFg5PilxKVmi2W3IuuiG0y9GTLrT_J8y23jPQZCvZnqJ_zeOAsbioZ-PKIaifIyxpuhgwRFw-3TICdsGtx7cg-19lujtCq3FPXR2-XdqQbbEP-ZpJMJh7HZYTcKm4G2kNyQN19ulinEwXkgMI7aptC-hELaPb7maZjPjuRBRJNA/file# [following]\n",
            "--2022-12-16 17:10:25--  https://uc1fe3baf1750fc0b91651b53016.dl.dropboxusercontent.com/cd/0/inline/ByvyFg5PilxKVmi2W3IuuiG0y9GTLrT_J8y23jPQZCvZnqJ_zeOAsbioZ-PKIaifIyxpuhgwRFw-3TICdsGtx7cg-19lujtCq3FPXR2-XdqQbbEP-ZpJMJh7HZYTcKm4G2kNyQN19ulinEwXkgMI7aptC-hELaPb7maZjPjuRBRJNA/file\n",
            "Resolving uc1fe3baf1750fc0b91651b53016.dl.dropboxusercontent.com (uc1fe3baf1750fc0b91651b53016.dl.dropboxusercontent.com)... 162.125.81.15, 2620:100:6031:15::a27d:510f\n",
            "Connecting to uc1fe3baf1750fc0b91651b53016.dl.dropboxusercontent.com (uc1fe3baf1750fc0b91651b53016.dl.dropboxusercontent.com)|162.125.81.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BysD-cqVzoz3pApGQMWG5pDWO7akaBrJWbuANzGArGTo-1Jbg8fMiK0KIUD6OOEDxI5cxxK9PsxMwzx1cBD9cghnAgYEuywI_30BJPPaA_TIU10xWbOPSNMUJkYMVSgEmKKvVH3v8BTAQk4-xMLZIfOpA_LV3KQhNCCMlDzobkQIa_h3YFrinblQdRa7t7DbeiOA-TB6mpzIBt9pj3GdfP9d1Twwr9wV5cS1-oCpN-eYO9LDIxmwkviX87cwvIgTJ53hIUpGPxSm6hXXYRoMhtrPnHoojwB1DiZtRjEfmZhRv1xgb6Eflx5Ed7pYw-10xQcpOVO9yQpZT5Ha1OGaYKGpUD9l45vhNVt2dNqFXIa26Z8eMDftexRAf-TaQ3AZrx0cH4Jb8i0_m4p8fwUx9WMzT6Xz8Yv6dpscd74itjP3tA/file [following]\n",
            "--2022-12-16 17:10:25--  https://uc1fe3baf1750fc0b91651b53016.dl.dropboxusercontent.com/cd/0/inline2/BysD-cqVzoz3pApGQMWG5pDWO7akaBrJWbuANzGArGTo-1Jbg8fMiK0KIUD6OOEDxI5cxxK9PsxMwzx1cBD9cghnAgYEuywI_30BJPPaA_TIU10xWbOPSNMUJkYMVSgEmKKvVH3v8BTAQk4-xMLZIfOpA_LV3KQhNCCMlDzobkQIa_h3YFrinblQdRa7t7DbeiOA-TB6mpzIBt9pj3GdfP9d1Twwr9wV5cS1-oCpN-eYO9LDIxmwkviX87cwvIgTJ53hIUpGPxSm6hXXYRoMhtrPnHoojwB1DiZtRjEfmZhRv1xgb6Eflx5Ed7pYw-10xQcpOVO9yQpZT5Ha1OGaYKGpUD9l45vhNVt2dNqFXIa26Z8eMDftexRAf-TaQ3AZrx0cH4Jb8i0_m4p8fwUx9WMzT6Xz8Yv6dpscd74itjP3tA/file\n",
            "Reusing existing connection to uc1fe3baf1750fc0b91651b53016.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21340222 (20M) [application/zip]\n",
            "Saving to: ‘released_data.zip’\n",
            "\n",
            "released_data.zip   100%[===================>]  20.35M  14.3MB/s    in 1.4s    \n",
            "\n",
            "2022-12-16 17:10:27 (14.3 MB/s) - ‘released_data.zip’ saved [21340222/21340222]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js4124anRYJO"
      },
      "source": [
        "# Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwjtWmOdRQeR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a5b3cb-486c-4869-e291-a7f0d3a3ef18"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "device = 'cpu'\n",
        "if torch.cuda.is_available():\n",
        "    device = 'cuda'\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-aRHmZaXYkm"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_dbmTg1XdUa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        },
        "outputId": "4abf9a20-358d-4b88-c5e3-07d6240d5ca2"
      },
      "source": [
        "'''\n",
        "base_dir = r'/content/drive/MyDrive/fruits-360/Training/'\n",
        "train_test_split = 0.7\n",
        "no_of_files_in_each_class = 10\n",
        "\n",
        "#Read all the folders in the directory\n",
        "folder_list = os.listdir(base_dir)\n",
        "print( len(folder_list), \"categories found in the dataset\")\n",
        "\n",
        "#Declare training array\n",
        "cat_list = []\n",
        "x = []\n",
        "y = []\n",
        "y_label = 0\n",
        "\n",
        "#Using just no_of_files_in_each_class images per category\n",
        "for folder_name in folder_list:\n",
        "    files_list = os.listdir(os.path.join(base_dir, folder_name))\n",
        "    if len(files_list) < no_of_files_in_each_class:\n",
        "      print(f\"skipping {folder_name}\")\n",
        "      continue\n",
        "    temp=[]\n",
        "    for file_name in files_list[:no_of_files_in_each_class]:\n",
        "        temp.append(len(x))\n",
        "        x.append(np.asarray(Image.open(os.path.join(base_dir, folder_name, file_name)).convert('RGB').resize((100, 100))))\n",
        "        y.append(y_label)\n",
        "    y_label+=1\n",
        "    cat_list.append(temp)\n",
        "\n",
        "cat_list = np.asarray(cat_list)\n",
        "x = np.asarray(x)/255.0\n",
        "y = np.asarray(y)\n",
        "print('X, Y shape',x.shape, y.shape, cat_list.shape)\n",
        "'''\n",
        "\n",
        "\n",
        "full_imgs = np.array(os.listdir('./train_shuffle'))\n",
        "full_labels = pd.read_csv('./Released_Data/train_data.csv').to_numpy()\n",
        "\n",
        "labels_list = [set(),set(),set()]\n",
        "\n",
        "#prepare lists based on the super classes\n",
        "labels_list = [{}, {}, {}]\n",
        "val_labels_list = [{}, {}, {}]\n",
        "\n",
        "for i in range(len(full_labels)):\n",
        "    if full_labels[i][2] in labels_list[full_labels[i][1]]:\n",
        "        labels_list[full_labels[i][1]][full_labels[i][2]] += 1\n",
        "    else:\n",
        "        labels_list[full_labels[i][1]][full_labels[i][2]] = 1\n",
        "        val_labels_list[full_labels[i][1]][full_labels[i][2]] = 0\n",
        "\n",
        "# Check the number of times each sub class appears\n",
        "# print(dict(sorted(labels_list[0].items(), key=lambda item: item[1])))\n",
        "# print(dict(sorted(labels_list[1].items(), key=lambda item: item[1])))\n",
        "# print(dict(sorted(labels_list[2].items(), key=lambda item: item[1])))\n",
        "\n",
        "#Splitting the train and validation datasets (90 - 10 split)\n",
        "train_imgs = []\n",
        "val_imgs = []\n",
        "\n",
        "for i in range(len(full_labels)):\n",
        "    if val_labels_list[full_labels[i][1]][full_labels[i][2]] <= (0.1)*(labels_list[full_labels[i][1]][full_labels[i][2]]):\n",
        "        val_labels_list[full_labels[i][1]][full_labels[i][2]] += 1\n",
        "        val_imgs.append(full_labels[i][0])\n",
        "    else:\n",
        "        train_imgs.append(full_labels[i][0])\n",
        "\n",
        "#Check if there is any common image between training and validation datasets\n",
        "common_names = [name for name in train_imgs if name in val_imgs]\n",
        "print(\"Common names: \", common_names)\n",
        "\n",
        "train_data = full_labels[np.isin(full_labels[:, 0], train_imgs)]\n",
        "val_data = full_labels[np.isin(full_labels[:, 0], val_imgs)]\n",
        "\n",
        "#Check if there is any name missing from the training dataset\n",
        "ver_train_names = train_data[:, 0]\n",
        "error_names = [name for name in ver_train_names if name not in train_imgs]\n",
        "print(\"Error names: \", error_names)\n",
        "\n",
        "print(\"Total number of images: \", len(full_imgs))\n",
        "print(\"Number of training images: \", train_data.shape)\n",
        "print(\"Number of validation images: \", val_data.shape)\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "full_test_imgs = os.listdir(\"./test_shuffle\")\n",
        "\n",
        "for i in range(len(train_data)):\n",
        "    if train_data[i][1] == 0:\n",
        "        train_img = cv2.imread(os.path.join('train_shuffle', train_data[i][0]), cv2.IMREAD_UNCHANGED)\n",
        "        train_img = cv2.resize(train_img, (64, 64), interpolation = cv2.INTER_CUBIC)\n",
        "        cv2.imwrite(\"sample_bird.png\", train_img)\n",
        "    \n",
        "    if train_data[i][1] == 2:\n",
        "        train_img = cv2.imread(os.path.join('train_shuffle', train_data[i][0]), cv2.IMREAD_UNCHANGED)\n",
        "        train_img = cv2.resize(train_img, (64, 64), interpolation = cv2.INTER_CUBIC)\n",
        "        cv2.imwrite(\"sample_reptile.png\", train_img)\n",
        "\n",
        "train_img = cv2.imread(os.path.join('train_shuffle', train_data[0][0]), cv2.IMREAD_UNCHANGED)\n",
        "train_img = cv2.resize(train_img, (64, 64), interpolation = cv2.INTER_CUBIC)\n",
        "cv2.imwrite(\"sample_dog.png\", train_img)\n",
        "print(train_data[0][0], train_data[0][1], train_data[0][2], train_img.shape)\n",
        "transform_data = A.Compose([A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1)])\n",
        "train_img = transform_data(image = train_img)['image']\n",
        "print(type(train_img), train_img.shape)\n",
        "#print(np.unique(train_img))\n",
        "cv2_imshow(train_img)\n",
        "\n",
        "val_img = cv2.imread(os.path.join('train_shuffle', val_data[0][0]), cv2.IMREAD_UNCHANGED)\n",
        "val_img = cv2.resize(val_img, (64, 64), interpolation = cv2.INTER_CUBIC)\n",
        "print(val_data[0][0], val_data[0][1], val_data[0][2], val_img.shape)\n",
        "cv2.imwrite(\"sample_val.png\", val_img)\n",
        "cv2_imshow(val_img)\n",
        "\n",
        "test_img = cv2.imread(os.path.join('test_shuffle', full_test_imgs[0]), cv2.IMREAD_UNCHANGED)\n",
        "test_img = cv2.resize(test_img, (64, 64), interpolation = cv2.INTER_CUBIC)\n",
        "print(full_test_imgs[0], test_img.shape)\n",
        "cv2.imwrite(\"sample_test.png\", test_img)\n",
        "cv2_imshow(test_img)\n",
        "\n",
        "\n",
        "class KaggleDataset(nn.Module):\n",
        "    def __init__(self, mode = 'train'):\n",
        "        super(KaggleDataset, self).__init__()\n",
        "        self.mode = mode\n",
        "\n",
        "        if mode == 'train':\n",
        "            self.dataset = train_data\n",
        "        else:\n",
        "            self.dataset = val_data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = self.dataset[idx][0]\n",
        "        img = cv2.imread(os.path.join('train_shuffle', img_name))\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)    #convert image from BGR to RGB format\n",
        "\n",
        "        super_class = torch.tensor(self.dataset[idx][1], dtype = torch.float32)\n",
        "        sub_class = torch.tensor(self.dataset[idx][2], dtype = torch.float32)\n",
        "\n",
        "        apply_transform = self.transform_data()\n",
        "        image = apply_transform(image = img)['image']\n",
        "\n",
        "        return image, super_class, sub_class\n",
        "\n",
        "    def transform_data(self):\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            transform_data = A.Compose(\n",
        "              [\n",
        "                  #always resize the image to 329x224\n",
        "                  #A.Resize(height = 329, width= 224, interpolation = cv2.INTER_AREA, p=1),\n",
        "                  A.HorizontalFlip(p=0.4),  \n",
        "                  A.ShiftScaleRotate(shift_limit=0.025, scale_limit=0, rotate_limit=15, p=0.5),\n",
        "                  #A.RandomCrop(height = 224, width = 224, p=1),\n",
        "                  #randomly change brightness, contrast, and saturation of the image 50% of the time\n",
        "                  A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue = 0, p=0.5), \n",
        "                  A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1), \n",
        "                  ToTensorV2(p=1),\n",
        "              ])\n",
        "        else:     #augmentations during validation and testing\n",
        "          transform_data = A.Compose(\n",
        "          [\n",
        "              #always resize the image to 329x224\n",
        "              #A.Resize(height = 329, width = 224, p=1),   \n",
        "              A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1),\n",
        "              ToTensorV2(p=1),\n",
        "          ])\n",
        "    \n",
        "        return transform_data"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common names:  []\n",
            "Error names:  []\n",
            "Total number of images:  6472\n",
            "Number of training images:  (5738, 3)\n",
            "Number of validation images:  (734, 3)\n",
            "328.jpg 1 78 (64, 64, 3)\n",
            "<class 'numpy.ndarray'> (64, 64, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FAA4409CCD0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAAB30lEQVR4nO1ZSZLDIAxErvz/y8xhQtkGoZ3FKfchkARktRYEGNIFkHhkwRhaFCtBNfcQqbMxHk/g899IgieJ42cmDhBrPw0qMylCaEPzJzmBydr3HtcGi4jAEttL12s2AWza98RqpbHqHYnM4uVxzypQQmjDxaiA5vCMQkZwmEpgREB+1oe5D6M80JplkKEmhZBTe2J9GVUHYkEREI5eSIM2cfffqFLqRzCBFrkMHsGNVYMaYCjN4RwqHZ59JkatszuB3HQqRIbQkpVqdw+wCCOwqlD8tAd22ESwYDwg5LCQKh9CeW9X1LtRQlfPhfM41B7Y9WTfRcDV4toA++ll9BGoCdDxsOFyFOCBtXl/I6AzMKBdI8wSdBMB6X15e6LLcyI1hRAgXyeZsIEzB+DaLkkGDwEon6f6Wg5+znoCgD7azsGJwEJW1Iepb0v0BKjFQpcSISTNHiCuOaa6Qi3/kgLii+2mULQzzWXEQ0A4PVctOUiN80QmF9HjQIpiynUAAZUUNJC402n3B8825JbEpoTLSE83zwXjKpSxJgm1ut9zeF+fyW8lkMlY00GdA1FnI0sSn5PrHvleyr3rZnRwzaczeYzq6PN8Ijr1aehJOqbS7/Zm1o7HXey9ePHihRN/rbhbcDIz9w4AAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.jpg 1 63 (64, 64, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FAA4409CA00>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9066.jpg (64, 64, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7FAA4409CD30>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bfdur2NiUyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02eff725-8db4-4b41-f04c-266b5146ad67"
      },
      "source": [
        "# Adapt x input dimension to PyTorch format.\n",
        "'''\n",
        "x = x.transpose(0, 3, 1, 2)\n",
        "print('X, Y shape',x.shape, y.shape, cat_list.shape)\n",
        "'''\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "train_dataset = KaggleDataset(mode='train')\n",
        "val_dataset = KaggleDataset(mode='val')\n",
        "    \n",
        "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True, num_workers = 8, pin_memory = True)\n",
        "val_loader = DataLoader(dataset = val_dataset, batch_size = 64, shuffle = False)\n",
        "\n",
        "\n",
        "# print(len(train_loader))\n",
        "# print(len(val_loader))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhS_zspMYXyb"
      },
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIRD6DN3YPyA"
      },
      "source": [
        "'''\n",
        "train_size = int(len(folder_list)*train_test_split)\n",
        "test_size = len(folder_list) - train_size\n",
        "print(train_size, 'classes for training and', test_size, ' classes for testing')\n",
        "\n",
        "train_files = train_size * no_of_files_in_each_class\n",
        "\n",
        "#Training Split\n",
        "x_train = x[:train_files]\n",
        "y_train = y[:train_files]\n",
        "cat_train = cat_list[:train_size]\n",
        "\n",
        "#Validation Split\n",
        "x_val = x[train_files:]\n",
        "y_val = y[train_files:]\n",
        "cat_test = cat_list[train_size:]\n",
        "\n",
        "print('X&Y shape of training data :',x_train.shape, 'and', y_train.shape, cat_train.shape)\n",
        "print('X&Y shape of testing data :' , x_val.shape, 'and', y_val.shape, cat_test.shape)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuACgkS4YlhM"
      },
      "source": [
        "## Generating Batch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1k-slYI3Yo2D"
      },
      "source": [
        "'''\n",
        "def get_batch(batch_size=64):\n",
        "    \n",
        "    temp_x = x_train\n",
        "    temp_cat_list = cat_train\n",
        "    start=0\n",
        "    end=train_size\n",
        "    batch_x=[]\n",
        "        \n",
        "    batch_y = np.zeros(batch_size)\n",
        "    batch_y[int(batch_size/2):] = 1\n",
        "    np.random.shuffle(batch_y)\n",
        "    \n",
        "    class_list = np.random.randint(start, end, batch_size) \n",
        "    batch_x.append(np.zeros((batch_size, 3, 100, 100)))\n",
        "    batch_x.append(np.zeros((batch_size, 3, 100, 100)))\n",
        "\n",
        "    for i in range(0, batch_size):\n",
        "        batch_x[0][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]\n",
        "        #If train_y has 0 pick from the same class, else pick from any other class\n",
        "        if batch_y[i]==0:\n",
        "            batch_x[1][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]\n",
        "\n",
        "        else:\n",
        "            temp_list = np.append(temp_cat_list[:class_list[i]].flatten(), temp_cat_list[class_list[i]+1:].flatten())\n",
        "            batch_x[1][i] = temp_x[np.random.choice(temp_list)]\n",
        "            \n",
        "    return(batch_x, batch_y)\n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5gplqrZY7FT"
      },
      "source": [
        "## Siamese Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcnSmw5aY9n1"
      },
      "source": [
        "#Building a sequential model\n",
        "class CnnNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CnnNetwork, self).__init__()\n",
        "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
        "        # kernel\n",
        "        self.conv1 = nn.Conv2d(3, 64, 10)\n",
        "        self.conv2 = nn.Conv2d(64, 128, 7)\n",
        "        self.conv3 = nn.Conv2d(128, 128, 4)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 4)\n",
        "        # an affine operation: y = Wx + b\n",
        "        self.fc1 = nn.Linear(256 * 4, 1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Max pooling over a (2, 2) window\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
        "        # If the size is a square, you can specify with a single number\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv4(x)), 2)\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.sigmoid(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
        "        num_features = 1\n",
        "        for s in size:\n",
        "            num_features *= s\n",
        "        return num_features\n",
        "\n",
        "\n",
        "class SiameseNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SiameseNetwork, self).__init__()\n",
        "        self.cnn = CnnNetwork()\n",
        "        self.fc1 = nn.Linear(1024, 1)\n",
        "\n",
        "    def forward(self, left, right):\n",
        "        x = self.cnn(left)\n",
        "        y = self.cnn(right)\n",
        "        diff = torch.abs(x - y)\n",
        "        # z = self.fc1(diff)\n",
        "        z = F.sigmoid(self.fc1(diff))\n",
        "        return z\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEN8hj5IeDgr"
      },
      "source": [
        "## N-way one-shot Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vb55CMwyeJha"
      },
      "source": [
        "def nway_one_shot(n_way, n_val):\n",
        "    \n",
        "    temp_x = x_val\n",
        "    temp_cat_list = cat_test\n",
        "    batch_x=[]\n",
        "    x_0_choice=[]\n",
        "    n_correct = 0\n",
        "   \n",
        "    class_list = np.random.randint(train_size+1, len(cat_list)-1, n_val)\n",
        "\n",
        "    for i in class_list:  \n",
        "        j = np.random.choice(cat_list[i])\n",
        "        temp=[]\n",
        "        temp.append(np.zeros((n_way, 3, 100, 100)))\n",
        "        temp.append(np.zeros((n_way, 3, 100, 100)))\n",
        "        for k in range(0, n_way):\n",
        "            temp[0][k] = x[j]\n",
        "            # 2 is arbitrary here, as 0 is the default number when all numbers\n",
        "            # are equal, which leads to wrong conclusions.\n",
        "            if k==2:\n",
        "                temp[1][k] = x[np.random.choice(cat_list[i])]\n",
        "            else:\n",
        "                temp[1][k] = x[np.random.choice(np.append(cat_list[:i].flatten(), cat_list[i+1:].flatten()))]\n",
        "\n",
        "        result = siamese_net(torch.Tensor(temp[0]).cuda(), torch.Tensor(temp[1]).cuda())\n",
        "        result = result.flatten().tolist()\n",
        "        result_index = result.index(min(result))\n",
        "        if result_index == 2:\n",
        "            n_correct = n_correct + 1\n",
        "    print(n_correct, \"correctly classified among\", n_val)\n",
        "    accuracy = (n_correct*100)/n_val\n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RzHZfladnDF1"
      },
      "source": [
        "# Tools to display batch data graphically\n",
        "\n",
        "def display_batch(batch_x, batch_y, batch_size=64):\n",
        "  num = int(batch_size ** 0.5)\n",
        "  combined_left = np.zeros((num*100, num*100, 3))\n",
        "  combined_right = np.zeros((num*100, num*100, 3))\n",
        "  count = 0\n",
        "  for i in range(num):\n",
        "    for j in range(num):\n",
        "      left_image = batch_x[0][count].transpose(1, 2, 0)\n",
        "      right_image = batch_x[1][count].transpose(1, 2, 0)\n",
        "      combined_left[i*100:(i+1)*100, j*100:(j+1)*100, :] = left_image\n",
        "      combined_right[i*100:(i+1)*100, j*100:(j+1)*100, :] = right_image\n",
        "      count += 1\n",
        "  plt.imshow(combined_left)\n",
        "  plt.show()\n",
        "  plt.imshow(combined_right)\n",
        "  plt.show()\n",
        "  print(\"batch_y is\")\n",
        "  print(np.reshape(batch_y, (-1, num)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JvNW3l5eW5F"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kFcTUnGeYYF"
      },
      "source": [
        "'''\n",
        "epochs = 10\n",
        "n_way = 20\n",
        "n_val = 100\n",
        "batch_size = 64\n",
        "\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "for epoch in range(epochs):\n",
        "    if epoch == 0:\n",
        "        accuracy = nway_one_shot(n_way, n_val)\n",
        "\n",
        "    batch_x, batch_y = get_batch(batch_size)\n",
        "    # display_batch(batch_x, batch_y)\n",
        "    # zero the parameter gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward + backward + optimize\n",
        "    siamese_outputs = siamese_net(torch.Tensor(batch_x[0]).cuda(), torch.Tensor(batch_x[1]).cuda())\n",
        "    outputs = loss(siamese_outputs, torch.Tensor(batch_y).reshape(64, 1).cuda())\n",
        "    outputs.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print('Epoch:', epoch, ', Loss:',outputs)\n",
        "    loss_list.append((epoch,outputs.item()))\n",
        "    # print statistics\n",
        "    if epoch % 250 == 0:\n",
        "        print(\"=============================================\")\n",
        "        accuracy = nway_one_shot(n_way, n_val)\n",
        "        accuracy_list.append((epoch, accuracy))\n",
        "        print('Accuracy as of', epoch, 'epochs:', accuracy)\n",
        "        print(\"=============================================\")\n",
        "        if(accuracy>90):\n",
        "            print(\"Achieved more than 90% Accuracy\")\n",
        "'''\n",
        "\n",
        "# Will contain utility functions used for training the model\n",
        "import torch\n",
        "import os\n",
        "import copy\n",
        "from tqdm import tqdm\n",
        "from time import sleep\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import time\n",
        "\n",
        "#Training Function\n",
        "def fit_classifier(model, train_loader, val_loader, optimizer, loss_func, epochs=10, initial_epoch=0, device='cpu', name='siamese'):\n",
        "    '''\n",
        "    function to train a classifier model.\n",
        "    args:\n",
        "        model - the model to be trained\n",
        "        train_loader - Dataloader() for train set\n",
        "        val_loader - Dataloader() for val set\n",
        "        optimizer - optimization algorithm for updating weights\n",
        "        loss_func - loss function to be used\n",
        "    \n",
        "    keyword args:\n",
        "        epochs - Number of training epochs (default=10)\n",
        "        initial_epoch - The starting epoch\n",
        "        device - the device for training (default='cpu')\n",
        "        name - Name for saving the model\n",
        "    \n",
        "    returns: Nothing\n",
        "    \n",
        "    '''\n",
        "    \n",
        "    model = model.to(device, non_blocking=True)\n",
        "    \n",
        "    # Save the models based on the super and sub class validation accuracies\n",
        "    best_super_acc = torch.tensor([0.]).to(device, non_blocking=True)\n",
        "    best_sub_acc = torch.tensor([0.]).to(device, non_blocking=True)\n",
        "    \n",
        "    #create the logger object\n",
        "    writer = SummaryWriter()\n",
        "    start_time = time.time()\n",
        "    \n",
        "    #Iterate epochs\n",
        "    for epoch in range(initial_epoch, initial_epoch + epochs):\n",
        "        #Each epoch has a training phase and validation phase\n",
        "        for phase in ['train','val']:\n",
        "            data_loader = None\n",
        "            if phase == 'train':\n",
        "                #Set train mode\n",
        "                model.train()\n",
        "                data_loader = train_loader\n",
        "            else:\n",
        "                #Set Eval mode\n",
        "                model.eval()\n",
        "                data_loader = val_loader\n",
        "          \n",
        "            running_super_loss = 0.\n",
        "            running_sub_loss = 0.\n",
        "            running_super_corrects = torch.tensor([0.]).to(device, non_blocking=True)\n",
        "            running_sub_corrects = torch.tensor([0.]).to(device, non_blocking=True)\n",
        "            \n",
        "            #tqdm for observing the progress\n",
        "            with tqdm(data_loader, unit=\"batch\") as tepoch:\n",
        "                #Iterate batches\n",
        "                for itr, (images, super_labels, sub_labels) in enumerate(tepoch):\n",
        "                    tepoch.set_description(f\"Epoch {(epoch)} {phase}\")\n",
        "                    images = images.to(device, non_blocking=True)\n",
        "\n",
        "                    super_labels = super_labels.long().to(device, non_blocking=True)\n",
        "                    sub_labels = sub_labels.long().to(device, non_blocking=True)\n",
        "                    \n",
        "                    optimizer.zero_grad()\n",
        "                    \n",
        "                    #Set gradient calculation only for training phase\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        #super_outputs, sub_outputs = model(images)\n",
        "                        sub_outputs = model(images)\n",
        "\n",
        "                        #super_loss = loss_func(super_outputs, super_labels)\n",
        "                        sub_loss = loss_func(sub_outputs, sub_labels)\n",
        "\n",
        "                        #super_preds = torch.argmax(super_outputs, dim=1)\n",
        "                        sub_preds = torch.argmax(sub_outputs, dim=1)\n",
        "\n",
        "                        #loss = 5 * super_loss + sub_loss\n",
        "                        loss = sub_loss\n",
        "                        \n",
        "                        #Do backprop only during training\n",
        "                        if phase == 'train':\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                    \n",
        "                    #running_super_loss += 5 * super_loss.item() * images.size(0)\n",
        "                    running_sub_loss += sub_loss.item() * images.size(0)\n",
        "                    #running_super_corrects += torch.sum(super_preds == super_labels)\n",
        "                    running_sub_corrects += torch.sum(sub_preds == sub_labels)\n",
        "                    \n",
        "                    if phase == 'train':\n",
        "                        writer.add_scalar(\"Batch_Loss/\" + phase, loss.item(), epoch * len(data_loader) + itr)\n",
        "                        # writer.add_scalar(\"Batch_Accuracy_Super_Class/\" + phase,\n",
        "                        #                   (torch.sum(super_preds == super_labels)/(images.shape[0])).item(),\n",
        "                        #                   epoch * len(data_loader) + itr)\n",
        "                        writer.add_scalar(\"Batch_Accuracy_Sub_Class/\" + phase,\n",
        "                                          (torch.sum(sub_preds == sub_labels)/(images.shape[0])).item(),\n",
        "                                          epoch * len(data_loader) + itr)\n",
        "                    \n",
        "                    tepoch.set_postfix(loss=loss.item(),\n",
        "                              #super_class_accuracy=(torch.sum(super_preds == super_labels)/(images.shape[0])).item(),\n",
        "                              sub_class_accuracy=(torch.sum(sub_preds == sub_labels)/(images.shape[0])).item())\n",
        "                \n",
        "                #epoch_super_loss = running_super_loss / len(data_loader.dataset)\n",
        "                epoch_sub_loss = running_sub_loss / len(data_loader.dataset)\n",
        "                #epoch_super_acc = running_super_corrects.float() / (len(data_loader.dataset))\n",
        "                epoch_sub_acc = running_sub_corrects.float() / (len(data_loader.dataset))\n",
        "\n",
        "                #print(f\"Epoch {(epoch)} {phase} Super Class loss: {epoch_super_loss} Super Class acc: {epoch_super_acc.item()} Sub Class loss: {epoch_sub_loss} Sub Class acc: {epoch_sub_acc.item()}\")\n",
        "                print(f\"Epoch {(epoch)} {phase} Sub Class loss: {epoch_sub_loss} Sub Class acc: {epoch_sub_acc.item()}\")\n",
        "                \n",
        "                #writer.add_scalar(\"Epoch_Loss_Super_Class/\" + phase, epoch_super_loss, epoch)\n",
        "                writer.add_scalar(\"Epoch_Loss_Sub_Class/\" + phase, epoch_sub_loss, epoch)\n",
        "                #writer.add_scalar(\"Epoch_Accuracy_Super_Class/\" + phase, epoch_super_acc, epoch)\n",
        "                writer.add_scalar(\"Epoch_Accuracy_Sub_Class/\" + phase, epoch_sub_acc, epoch)\n",
        "                \n",
        "                # #Saving best model based on super class accuracy\n",
        "                # if phase == 'val' and epoch_super_acc > best_super_acc:\n",
        "                #     best_super_acc = epoch_super_acc\n",
        "                #     os.makedirs('./models', exist_ok = True)\n",
        "                #     torch.save({      \n",
        "                #         'epoch': epoch,\n",
        "                #         'model_state_dict': model.state_dict(),\n",
        "                #     }, f\"./models/{name}_SuperClass.pth\")\n",
        "\n",
        "                #Saving best model based on sub class accuracy\n",
        "                if phase == 'val' and epoch_sub_acc > best_sub_acc:\n",
        "                    best_sub_acc = epoch_sub_acc\n",
        "                    os.makedirs('./models', exist_ok = True)\n",
        "                    torch.save({      \n",
        "                        'epoch': epoch,\n",
        "                        'model_state_dict': model.state_dict(),\n",
        "                    }, f\"./models/{name}_SubClass.pth\")    \n",
        "                \n",
        "        print('-'*20)\n",
        "    \n",
        "    #End of Training \n",
        "    end_time = time.time()  \n",
        "    writer.close()\n",
        "    #print('Best Super Class val acc: {}'.format(best_super_acc.item()))\n",
        "    print('Best Sub Class val acc: {}'.format(best_sub_acc.item()))\n",
        "    print(f\"Average Time taken for an epoch: {(end_time - start_time)/epochs} sec\")\n",
        "    \n",
        "    return"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train the model"
      ],
      "metadata": {
        "id": "8QSV3q7GvSOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "\n",
        "epochs = 5\n",
        "initial_epoch = 0\n",
        "learning_rate = 0.0001\n",
        "siamese_net = SiameseNetwork().cuda()\n",
        "#optimizer = torch.optim.AdamW(trainable_params, lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(siamese_net.parameters(), lr=learning_rate)\n",
        "#loss_func = torch.nn.CrossEntropyLoss()\n",
        "loss_func = torch.nn.BCELoss()\n",
        "\n",
        "\n",
        "fit_classifier(\n",
        "    siamese_net, \n",
        "    train_loader=train_loader, \n",
        "    val_loader=val_loader,\n",
        "    optimizer=optimizer, \n",
        "    loss_func=loss_func, \n",
        "    epochs=epochs,\n",
        "    initial_epoch=initial_epoch, \n",
        "    device=device,\n",
        "    name='Baseline_Siamese'\n",
        ")\n",
        "\n",
        "print(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "M0bLu1LGvWgv",
        "outputId": "d9bd008e-96f9-402d-c7ec-52083aab1e54"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/90 [00:00<?, ?batch/s]/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "Epoch 0 train:   0%|          | 0/90 [00:01<?, ?batch/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2a1c489ddf71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m fit_classifier(\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0msiamese_net\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-c42e1e7b9f03>\u001b[0m in \u001b[0;36mfit_classifier\u001b[0;34m(model, train_loader, val_loader, optimizer, loss_func, epochs, initial_epoch, device, name)\u001b[0m\n\u001b[1;32m    109\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                         \u001b[0;31m#super_outputs, sub_outputs = model(images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m                         \u001b[0msub_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                         \u001b[0;31m#super_loss = loss_func(super_outputs, super_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'right'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbhsaOT7C2XL",
        "outputId": "2f556b76-9f8f-416c-a9d0-d885fb4d171c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<torch.utils.data.dataloader.DataLoader object at 0x7faa2a9df460>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "boX_f-eVC3g5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}